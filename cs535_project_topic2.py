# -*- coding: utf-8 -*-
"""CS535_Project_Topic2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DcetcjhGkiK-JSLdIetaQFCZQmavhsmh

-Team number: 27

-Team Members: Teresa Chu, Kaushal Thakar, Kishore Kumar Basam, Vikas Kiran Nadikuda, Sathwik Krishtipati

## Problem 1: What is the problem that the paper aims to solve, and why is this problem important or interesting? Answer this question in two sentences. (5 points)

The paper aims to solve the problem of image generation by creating a model which is able to generate data or images from complete noise by using diffusion probabilistic models to remove predicted noise in an image at each time step. This problem is important because it allows us compared to other generative models, using a DDPM allows us to generate images with a limited quanity of training images.

## Implement DDPM and test it on 2-dimensional Swiss roll data. You should output Fig.3 of the blog intro 2. (20 points)

Note: 
- Code for model and variable extration references https://github.com/acids-ircam/diffusion_models in understanding model/neural network type and obtaining variables that can be used to predict 2D points
- Code for training and sampling references the paper https://hojonathanho.github.io/diffusion/assets/denoising_diffusion20.pdf
"""

#import libraries
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader

import torchvision
from torchvision import transforms
import torchvision.datasets as datasets

import numpy as np
import matplotlib.pyplot as plt
import math
import sklearn
import random

from PIL import Image
from tqdm import tqdm

# Get variables for algorithms
T = 300        
batch_size = 64  #Get multiple imgs per batch for each iteration but keep low else memory issues
mnist_batch_size = 8
img_size = 64   #If >128 time for loss backward takes min 70s per so takes a couple hrs to run
                #Note: img_size = 64 possible to run but still takes ~7s so still takes a while 
                #If not on collab or multiple gpu cores --> can try large img_size/batch_size

betas = torch.linspace(0.0001, 0.02, T)       #According to paper start = 10^-4 to 0.02
alphas = 1 - betas                            #Alphas = [1 - beta_i for i in range of len(betas)]
alphas_cumprod = torch.cumprod(alphas, axis=0)#Get alpha_i*alpha_i-1....alpha_0
sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)#Get sqrt of above

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu") #Use GPU to train if available

#Get a_t
def extract_from_list(torch_list, t, x_shape):
    out = torch_list.gather(-1, t.cpu())
    return out.reshape(t.shape[0], *((1,) * (len(x_shape) - 1))).to(device)

#Conversion to tensor from pil image
convertToTensor = transforms.Compose([
    transforms.Resize((img_size,img_size)), #Resize img 
    transforms.ToTensor(), #scale from [0,255] to [0,1] and changes channels
    transforms.Lambda(lambda x: x*2-1) #Scale to [-1,1]
  ])

# Display images in a row from batch of tensor
def display_imgs(imgs, size, title):
  if (type(imgs) == torch.Tensor):
    imgs = imgs.detach().cpu().numpy()  #detaching because of error without

  plt.figure(figsize=(16,16))

  for i in range(size):
    #Plot images in a row
    plt.subplot(1,size+1,i+1)
    plt.axis('off')
    plt.imshow(imgs[i][0],cmap= "gray")
  
  plt.title(title)

#Create DDPM for 2D points
class Model(nn.Module):
  def __init__(self,dim_in = 2, dim_out =  256, T_x = 100):
    super().__init__()
    #Variables
    self.dim_in = dim_in
    self.dim_out = dim_out
    self.T_x = T_x

    #Conv + Embd for points
    self.conv1 = nn.Linear(dim_in, dim_out)
    self.embed1 = nn.Embedding(T_x, dim_out)
    self.conv2 = nn.Linear(dim_out,dim_out)
    self.embed2 = nn.Embedding(T_x,dim_out)
    self.conv3 = nn.Linear(dim_out, dim_in)

  def forward(self,x,t):
    x = x.to(device)
    x = self.conv1(x) * self.embed1(t).view(-1,self.dim_out)
    x = F.softplus(x)
    x = self.conv2(x) * self.embed2(t).view(-1,self.dim_out)
    x = F.softplus(x)
    x = self.conv3(x)
    return x

def forward_transformation(x_tensor,t,noise = None):
  if noise == None:
    noise = torch.randn_like(x_tensor).to(device)

  #Get alphas cumprod for t and 1- alphas cumprod for the forward equation
  x_tensor = x_tensor.to(device)
  sqrt_alphas_cumprod_t = extract_from_list(torch.sqrt(alphas_cumprod), t, x_tensor.shape)
  sqrt_one_minus_alphas_cumprod_t = torch.sqrt(1 - sqrt_alphas_cumprod_t).to(device)

  #Apply function sqrt(alpha cumprod) * x_0 + sqrt(1-alpha cumprod)*e
  return sqrt_alphas_cumprod_t * x_tensor + sqrt_one_minus_alphas_cumprod_t * noise

@torch.no_grad()
def backward_transformation(model, x = None, shape = None):
    if x == None and shape == None:
      print("Missing initial tensor/tenso shape to denoise")
      return

    if x == None:
      x = torch.randn(shape)

    x = x.to(device)
    stored_results = []

    for i in tqdm(range(model.T_x-1,0,-1)):
      t = torch.Tensor([i]*x.shape[0]).to(device).long()
      predicted_noise =  model(x,t)

      #Get Variables for algorithm
      alpha_t = extract_from_list(alphas,t,x.shape)
      sqrt_one_over_alpha_t = torch.sqrt(1.0/alpha_t).to(device)
      sqrt_one_minus_alphas_cumprod_t = extract_from_list(torch.sqrt(1. - alphas_cumprod), t, x.shape).to(device)

      alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)
      pos_var_t = extract_from_list(betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod),t,x.shape)
      pos_var_t_sqrt = torch.sqrt(pos_var_t).to(device)

      #Remove noise
      x = (1/torch.sqrt(alpha_t))*(x- ((1-alpha_t)/sqrt_one_minus_alphas_cumprod_t)*predicted_noise)

      #Add in variance * gaussian noise if > 1
      if i > 1:
        z = torch.randn_like(x).to(device)
        x += (pos_var_t_sqrt *z)
      if ((i+1) % 50 == 0) or i ==1:
        stored_results.append(x)
      
    return stored_results

def drifting(model,x):
  x = x.to(device)
  stored_results = []

  for i in tqdm(range(0,model.T_x)):
    t = torch.Tensor([i]*x.shape[0]).to(device).long()
    predicted_noise =  model(x,t)

    #Get Variables for algorithm
    alpha_t = extract_from_list(alphas,t,x.shape)
    sqrt_one_minus_alphas_cumprod_t = extract_from_list(torch.sqrt(1. - alphas_cumprod), t, x.shape).to(device)

    #Remove noise u(x_t,t) - x_t
    x = (1/torch.sqrt(alpha_t))*(x - ((1-alpha_t)/sqrt_one_minus_alphas_cumprod_t)*predicted_noise) - x
    
    if ((i+1) % 50 == 0) or i ==1:
      stored_results.append(x)

  return stored_results


from torch.optim import Adam

def swiss_training(model,dataset,):
  optimizer = Adam(model.parameters(), lr=1e-3)

  for t in tqdm(range(30_000)):
    #Noise
    noise = torch.randn_like(dataset).to(device)

    #Get noise added to batch of points
    tens = torch.randint(0, model.T_x-1,(dataset.shape[0],)).to(device).long()
    noisy_result = forward_transformation(dataset,tens,noise)

    #Calculate noise of noisy img with model at t
    model_noise = model(noisy_result,tens)
    
    # Compute the loss.
    loss = F.l1_loss(model_noise,noise)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

from sklearn.datasets import make_swiss_roll
import math

def create_swiss_dataset(size, noise=0.05):
    data, shape = make_swiss_roll(size, noise=noise)
    data = data[:, [0, 2]] /8 #get x,y dimension and get rid of z dimension and divide to get to noise in forward transformation

    for i,(x,y) in enumerate(data): #Rotate to get points
      radians = math.radians(100)
      newx = x * math.cos(radians) + y * math.sin(radians)
      newy = -x * math.sin(radians) + y * math.cos(radians)
      data[i] = [newx,newy] #Rotate 100 degrees to get same direction as in paper
    return data 

def swiss_forward_scattered(dataset):
    n_range = 100
    num_images = 50
    n_indexes= int(n_range/num_images)

    fig, axs = plt.subplots(1, n_indexes+1, figsize=(10, 3))
    titles = ["t=0","t=T/2","t=T"]
    index = 0
    for i in range(0,101,50): 
      q_i = forward_transformation(dataset, torch.tensor([i])).cpu() 
      axs[index].scatter(q_i[:, 0], q_i[:, 1],s=5);
      axs[index].set_title(titles[index])
      index +=1
    return q_i

def swiss_backward_scattered(model, points):
    fig, axs = plt.subplots(1, 3, figsize=(10, 3))
    titles = ["t=0","t=T/2","t=T"]
    q_i = backward_transformation(model,points)
    index = 0
    for i in range(0,101,50): 
      backward_result = q_i[index].cpu()
      axs[index].scatter(backward_result[:, 0], backward_result[:, 1],s=5);
      axs[index].set_title(titles[index])
      index+=1
    return q_i

#Create swiss data
swissdata = create_swiss_dataset(1500).T
swiss_dataset = torch.tensor(swissdata.T).float()

#Create swiss models for 2d data
swiss_roll_model = Model()
swiss_roll_model.to(device)

#Training
swiss_training(swiss_roll_model,swiss_dataset)

#Disaply forward, backward and driftiing 
forward_result= swiss_forward_scattered(swiss_dataset)
denoise_result = swiss_backward_scattered(swiss_roll_model,forward_result)

"""## Test DDPM on the mixture of 8 Gaussians and 25 Gaussians respectively. These Gaussians can be either identical or with different parameters at your choice. You should output figures like those in Problem 2. (30 points)"""

from sklearn.mixture import GaussianMixture
from sklearn.datasets import make_blobs

#Create data
blob_data8, blob_labels = make_blobs(n_samples=800, centers=8,cluster_std=0.6, random_state=0)
blob_data25, blob_labels = make_blobs(n_samples=2500, centers=25,cluster_std=0.6, random_state=0)
blob_data8 = blob_data8/8
blob_data25 = blob_data25/8

gmm8 = GaussianMixture(n_components=8).fit(blob_data8)
gmm25 = GaussianMixture(n_components=25).fit(blob_data25)

e_dataset = torch.tensor(blob_data8.T.T).float()
tf_dataset = torch.tensor(blob_data25.T.T).float()

#Create models for 2d data
gaussian8_model = Model()
gaussian8_model.to(device)
gaussian25_model = Model()
gaussian25_model.to(device)

#Train 8 gaussian and 25 gaussian
#Training
swiss_training(gaussian8_model,e_dataset)
swiss_training(gaussian25_model,tf_dataset)

#Disaply forward, backward and driftiing of 8 gaussian
g8_forward_result= swiss_forward_scattered(e_dataset)
g8_denoise_result = swiss_backward_scattered(gaussian8_model,g8_forward_result)

#Disaply forward, backward and driftiing of 25 gaussian
g25_forward_result= swiss_forward_scattered(tf_dataset)
g25_denoise_result = swiss_backward_scattered(gaussian25_model,g25_forward_result)

"""## Test DDPM on MNIST dataset. Visualize the denoising process (You should output a figure like Fig. 6 in the paper). (30 points)

Note: Code below references github reference codes provided in helping with unet architecture and embeddings that include noise at time t. Code was used for understanding on implementation structure. These include:
- https://github.com/hojonathanho/diffusion/blob/master/diffusion_tf/diffusion_utils.py
- https://github.com/lucidrains/denoising-diffusion-pytorch/blob/main/denoising_diffusion_pytorch/denoising_diffusion_pytorch.py
- https://github.com/pesser/pytorch_diffusion/blob/master/pytorch_diffusion/model.py
"""

#Add in time embeddings for unet to denoise at time t
class SinusodualTimeEmbeddings(nn.Module):
  def __init__(self, time_d):
    super().__init__()
    self.d = time_d

  def forward(self, t):
    t = t.to(device)  #use GPU if possible
    half_d = self.d // 2
    emb = -math.log(10_000) / half_d
    emb = torch.exp(torch.arange(half_d) * emb).to(device)
    emb = t[:, None] * emb[None, :]
    emb = torch.cat([emb.sin(), emb.cos()], dim =-1)
    return emb

#Create block modules
class Block(nn.Module):
  def __init__(self, in_channels, out_channels, time = 32):
    super().__init__() 

    self.in_channels = in_channels
    self.out_channels = out_channels
    self.time_emb = time

    #Conv2d, normalize and then Relu --> Apply upsampling/downsampling outside of block (Unet architecture 2 conv )
    self.conv1 = nn.Sequential(
                    nn.Conv2d(in_channels, out_channels, kernel_size = 3, padding = 1),
                    nn.ReLU(),
                    nn.BatchNorm2d(out_channels),
                    )
    self.mlp = nn.Sequential(
        nn.Linear(time, out_channels).to(device),
        nn.ReLU()
    )
    self.conv2 = nn.Sequential(
                    nn.Conv2d(out_channels, out_channels, kernel_size = 3, padding = 1),
                    nn.ReLU(),
                    nn.BatchNorm2d(out_channels),
                    nn.ReLU())

  def forward(self,x,t):
    x = x.to(device)
    x = self.conv1(x)
    t = self.mlp(t)
    x += t[(..., ) + (None,None)]
    x = self.conv2(x)
    return x

class Unet(nn.Module):
  def __init__(self, img_channels, down_ch = (64,128,256,512,1024), up_ch = (1024,512,256,128,64) , time_d = 32):
    super().__init__()
    self.channels = img_channels
    self.time_dim = time_d

    self.down_ch = down_ch
    self.up_ch = up_ch

    self.down = nn.ModuleList() #Blocks to downsample from 64->1024
    self.up = nn.ModuleList()   #Blocks to upsample from 1024 -> 64
    self.down_conv = nn.ModuleList()  #Apply down sampling portion after block forward
    self.up_conv = nn.ModuleList()  #Apply upsampling portion after block forward

    #Downsample blocks + downsampling
    for i in range(len(down_ch)-1):
      self.down.append(Block(down_ch[i],down_ch[i+1]))
      self.down_conv.append(nn.Conv2d(self.down_ch[i+1],self.down_ch[i+1], kernel_size = 4, stride = 2, padding = 1))
    
    #Upsampling block + upsampling
    for i in range(len(up_ch)-1):
      self.up.append(Block(2*up_ch[i],up_ch[i+1]))
      self.up_conv.append(nn.ConvTranspose2d(self.up_ch[i+1],self.up_ch[i+1], kernel_size = 4, stride = 2, padding = 1))

    #First and Last projection -> Incov + OutCov
    self.inconv = nn.Conv2d(img_channels,down_ch[0],kernel_size = 3, padding = 1)
    self.outcov = nn.Conv2d(up_ch[-1], img_channels, kernel_size = 1)

    # Time embeddings
    self.embd = nn.Sequential(
        SinusodualTimeEmbeddings(time_d),
        nn.Linear(time_d, time_d).to(device),
        nn.ReLU()
    )
    
  def forward(self,x,t):
    t = self.embd(t)
    samples = []

    #First conv
    x = self.inconv(x)

    #Downsampling
    for i in range(len(self.down)):
      x = self.down[i](x,t)
      x = self.down_conv[i](x)
      samples.append(x)

    #Upsampling
    for i in range(len(self.up)):
      prev = samples[-1-i]
      x = torch.cat([x, prev], dim=1)
      x = self.up[i](x,t)
      x = self.up_conv[i](x)

    #Last conv for output
    x = self.outcov(x)
    return x

class DDPM(nn.Module):
  def __init__(self, unet_model, img_channels = 1, mnist_bs = mnist_batch_size):
    super().__init__() 
    self.unet_model = unet_model
    self.img_channels = img_channels
    self.batch_size = mnist_bs

  #Apply forward transformation to batch
  def forward(self,x_tensor,t,noise = None):
    #Get alphas cumprod for t and 1- alphas cumprod for the forward equation
    sqrt_alphas_cumprod_t = extract_from_list(torch.sqrt(alphas_cumprod), t, x_tensor.shape)
    sqrt_one_minus_alphas_cumprod_t = torch.sqrt(1 - sqrt_alphas_cumprod_t).to(device)

    if noise == None:
      noise = torch.randn_like(x_tensor).to(device)

    x_tensor = x_tensor.to(device)
    
    #Apply function sqrt(alpha cumprod) * x_0 + sqrt(1-alpha cumprod)*e
    noisy_img = sqrt_alphas_cumprod_t * x_tensor + sqrt_one_minus_alphas_cumprod_t * noise
  
    return noisy_img

  #Use unet model to predict noise to remove it from image
  @torch.no_grad()
  def denoise_img(self, print_imgs = False, x = None, T_x = 300):
    if x == None:
      x = torch.randn(self.batch_size,self.img_channels,img_size,img_size).to(device)

    for i in tqdm(range(T_x-1,0,-1)):
      t = torch.Tensor([i]*x.shape[0]).to(device).long()
      predicted_noise = self.unet_model(x,t)

      #Get Variables for algorithm
      alpha_t = extract_from_list(alphas,t,x.shape)
      sqrt_one_over_alpha_t = torch.sqrt(1.0/alpha_t).to(device)
      sqrt_one_minus_alphas_cumprod_t = extract_from_list(torch.sqrt(1. - alphas_cumprod), t, x.shape).to(device)

      alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)
      pos_var_t = extract_from_list(betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod),t,x.shape)
      pos_var_t_sqrt = torch.sqrt(pos_var_t).to(device)

      #Remove noise
      x = (1/torch.sqrt(alpha_t))*(x- ((1-alpha_t)/sqrt_one_minus_alphas_cumprod_t)*predicted_noise)

      #Add in variance * gaussian noise if > 1
      if i > 1:
        z = torch.randn_like(x).to(device)
        x += (pos_var_t_sqrt *z)
        if (print_imgs == True) and ((i == T_x-1) or (i % 50 == 0)):
          display_imgs(x,mnist_batch_size, f"Removal {i}")
      
    return x

#Get MNIST train and test dataset
mnist_traindata = datasets.MNIST("/mnist", train=True, download=True, transform = convertToTensor)
mnist_testdata = datasets.MNIST("/mnist", train=False, download=True, transform = convertToTensor)

import os.path
from os import path

unet_model = Unet(1)
unet_model.to(device)
ddpm_model = DDPM(unet_model)
ddpm_model.to(device)

#Training with Adam optimizer
def training(ddpm, dataloader, location = None): 
  optimizer = Adam(ddpm.unet_model.parameters(), lr=0.001)

  for imgs in tqdm(dataloader):
    t = torch.randint(0,300,(mnist_batch_size,)).to(device).long()
    #Noise
    noise = torch.randn_like(imgs).to(device)

    #Get noisy image at time t
    noisy_img = ddpm(imgs,t,noise)

    #Calculate noise of noisy img with model
    model_noise = ddpm.unet_model(noisy_img,t)

    #Calculate loss btwn actual noise and model calculated noise

    loss = F.l1_loss(model_noise,noise)

    #Make adjustments based on the loss
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

  if location != None:
    torch.save(ddpm.unet_model.state_dict(), location)
    print("Model saved")

train_dataset = DataLoader(mnist_traindata, batch_size=mnist_batch_size, shuffle=True, drop_last=True)
test_dataset = DataLoader(mnist_testdata, batch_size=mnist_batch_size, shuffle=True, drop_last=True)

#Limit size of train and test dataset because training time extended on large dataset
trainiter = iter(train_dataset)
testiter = iter(test_dataset)
train_imgs = []

#Add images + Check batch imgs added correctly
for i in range(len(train_dataset)):
  imgs, labels = next(trainiter)
  train_imgs.append(imgs)

for i in range(len(test_dataset)):
  imgs, labels = next(testiter)
  train_imgs.append(imgs)

#NOTE: TRAINING REQUIRES RUNNING RESULTS MULTIPLE TIMES TO GET RESULTS - Rerun above to reshuffle images and then train again to avoid overfitting
#BEST RESULTS WHEN RUNNING BEFORE+THISSECTION 3-4 TIMES SO 9 TIMES TOTAL (1-2 hours running on GPU time)
for i in range(3):
  training(ddpm_model,train_imgs)

#Get sampling from noise and display final image
def test_sampling(ddpm, print_imgs = False):
  batch_imgs = ddpm.denoise_img(print_imgs = print_imgs)
  display_imgs(batch_imgs,mnist_batch_size,f"Sampling imgs")
  return batch_imgs

#Generation of images with training - see process
generated_imgs = test_sampling(ddpm_model,print_imgs = True)

"""## Problem 5: Perform an interpolation experiment with your trained model from Problem 4 (You should output a figure like Fig. 8 in the paper). (35 points)

Note: Code references algorithm described in https://hojonathanho.github.io/diffusion/assets/denoising_diffusion20.pdf : 4.4 Interpolation section
"""

img1_s = train_imgs[0][0]
img2_s = train_imgs[0][1]

def interpolate(ddpm,img1,img2):
  img1 = img1.to(device)
  img2= img2.to(device)
  interpolation_imgs = []
  for i in range(0,100,10):
    noise = torch.randn_like(img2).to(device)
    t = torch.Tensor([i]).to(device).long()

    #Mix images together after adding noise ie in paper: x¯t = (1 − λ)x0 + λx0'
    lambda_int = i/100
    mixed = (1-lambda_int)*img1 + lambda_int*ddpm(img2,t)

    #Denoise mixed image with noise 
    mixed = [mixed]
    mixed = torch.stack(mixed).float()
    img_result = ddpm.denoise_img(x=mixed)
    interpolation_imgs.append(img_result[0])
  interpolation_imgs = torch.stack(interpolation_imgs).float()
  display_imgs(interpolation_imgs, len(interpolation_imgs),"Interpolation Images")

interpolate(ddpm_model,img1_s,img2_s)